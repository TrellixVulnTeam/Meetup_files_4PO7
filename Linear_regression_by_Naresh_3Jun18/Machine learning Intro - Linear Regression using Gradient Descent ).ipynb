{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "Machine learning enables computers to act and make data-driven decisions rather than being explicitly programmed to carry out a certain task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of machine learning consists of supervised learning,unsupervised learning, and reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/IRL1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output.\n",
    "Y = f(X)\n",
    "\n",
    "Supervised learning problems can be further grouped into regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: \n",
    "A classification problem is when the output variable is a category, such as \"yes\" or \"no\" or \"spam\" and \"not spam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from  sklearn import tree\n",
    "#features=[[140,\"red\"],[145,\"red\"],[170,\"Yellow\"],[180,\"yellow\"]]\n",
    "features=[[140,1],[150,1],[170,0],[180,0]]\n",
    "#labels=[\"apple\",\"apple\",\"orange\",\"orange\"]\n",
    "labels=[1,1,0,0]\n",
    "clf=tree.DecisionTreeClassifier()\n",
    "clf=clf.fit(features,labels)\n",
    "print(clf.predict([[160,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression:\n",
    "Regressipon predicts a continuous target variable Y. It allows you to estimate a value, such as prices or weight, based on input data x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Level Idea:Based on the Lot of Inputs and outputs we found a equation or Function that maps the inputs to Outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/linear regression.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s suppose we want to model the above set of points with a line. To do this we’ll use the standard y = mx + b line equation where m is the line’s slope and b is the line’s y-intercept. To find the best line for our data, we need to find the best set of slope m and y-intercept b values.\n",
    "A standard approach to solving this type of problem is to define an error function (also called a cost function) that measures how “good” a given line is. This function will take in a (m,b) pair and return an error value based on how well the line fits our data. To compute this error for a given line, we’ll iterate through each (x,y) point in our data set and sum the square distances between each point’s y value and the candidate line’s y value (computed at mx + b). It’s conventional to square this distance to ensure that it is positive and to make our error function differentiable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/gradient_descent_example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "![title](images/CostFunction.PNG)\n",
    "![title](images/CostFunction1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/gradient_descent_error_surface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point in this two-dimensional space represents a line. The height of the function at each point is the error value for that line. You can see that some lines yield smaller error values than others (i.e., fit our data better). When we run gradient descent search, we will start from some location on this surface and move downhill to find the line with the lowest error.\n",
    "\n",
    "To run gradient descent on this error function, we first need to compute its gradient. The gradient will act like a compass and always point us downhill. To compute it, we will need to differentiate our error function. Since our function is defined by two parameters (m and b), we will need to compute a partial derivative for each. These derivatives work out to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/linear_regression_gradient1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration will update m and b to a line that yields slightly lower error than the previous iteration. The direction to move in for each iteration is calculated using the two partial derivatives from above.The learningRate variable controls how large of a step we take downhill during each iteration. If we take too large of a step, we may step over the minimum. However, if we take small steps, it will require many iterations to arrive at the minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe how the error changes as we move toward the minimum. A good way to ensure that gradient descent is working correctly is to make sure that the error decreases for each iteration. Below is a plot of error values for the first 100 iterations of the above gradient search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ll give an introduction to the Gradient Descent algorithm, and walk through an example that demonstrates how gradient descent can be used to solve machine learning problems such as linear regression.\n",
    "gradient descent is an algorithm that minimizes functions. Before Discussing about GradientDescent we will discuss Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484.586557408649 Iterations theta0 0.014547010110737297 theta1 0.7370702973591052\n",
      "457.8542575737673 Iterations theta0 0.02187396295959641 theta1 1.1067954543515157\n",
      "199.50998572553894 Iterations theta0 0.025579224321293136 theta1 1.2922546649131115\n",
      "134.50591058200533 Iterations theta0 0.027467789559144355 theta1 1.385283255651245\n",
      "118.14969342239947 Iterations theta0 0.028445071981738963 theta1 1.4319472323843205\n",
      "114.0341490603815 Iterations theta0 0.02896524076647862 theta1 1.4553540088980408\n",
      "112.99857731713661 Iterations theta0 0.0292561141260467 theta1 1.4670946177201354\n",
      "112.7379818756847 Iterations theta0 0.02943196916380713 theta1 1.4729832982243762\n",
      "112.67238435909097 Iterations theta0 0.029550129024383073 theta1 1.4759365618962286\n",
      "112.65585181499746 Iterations theta0 0.02963934787473239 theta1 1.4774173755483797\n",
      "112.65166489759584 Iterations theta0 0.029714049245227046 theta1 1.4781595857319891\n",
      "112.65058436150113 Iterations theta0 0.029781468199526522 theta1 1.4785313011122556\n",
      "112.65028544701505 Iterations theta0 0.02984523395633242 theta1 1.4787171706313593\n",
      "112.6501832029397 Iterations theta0 0.02990716698731024 theta1 1.4788098170256598\n",
      "112.65013044507197 Iterations theta0 0.029968180468920396 theta1 1.4788557012777626\n",
      "112.65009013922885 Iterations theta0 0.0300287324644578 theta1 1.4788781289278565\n",
      "112.65005296694635 Iterations theta0 0.030089052745485803 theta1 1.4788887903917065\n",
      "112.65001658353181 Iterations theta0 0.030149256568940068 theta1 1.4788935497608395\n",
      "112.64998039901862 Iterations theta0 0.030209401749465813 theta1 1.4788953485533445\n",
      "112.64994426496074 Iterations theta0 0.030269517287775917 theta1 1.478895662279729\n",
      "112.64990814400619 Iterations theta0 0.030329617731073667 theta1 1.4788952310786534\n",
      "112.64987202675677 Iterations theta0 0.030389710376460655 theta1 1.4788944262150017\n",
      "112.6498359108476 Iterations theta0 0.030449798884276465 theta1 1.4788934339209232\n",
      "112.6497997956837 Iterations theta0 0.030509885090605177 theta1 1.4788923476133156\n",
      "112.64976368111527 Iterations theta0 0.03056996991645638 theta1 1.4788912141515247\n",
      "112.64972756710466 Iterations theta0 0.03063005382382429 theta1 1.4788900570409158\n",
      "112.64969145364233 Iterations theta0 0.03069013704445399 theta1 1.4788888880721398\n",
      "112.6496553407261 Iterations theta0 0.030750219694594058 theta1 1.478887713159568\n",
      "112.64961922835515 Iterations theta0 0.030810301832558278 theta1 1.4788865352699392\n",
      "112.64958311652931 Iterations theta0 0.030870383487599434 theta1 1.4788853558914152\n",
      "112.64954700524882 Iterations theta0 0.03093046467439288 theta1 1.4788841757704807\n",
      "112.64951089451318 Iterations theta0 0.0309905454003017 theta1 1.4788829952815838\n",
      "112.64947478432276 Iterations theta0 0.031050625669021042 theta1 1.4788818146125533\n",
      "112.64943867467751 Iterations theta0 0.03111070548240615 theta1 1.478880633857607\n",
      "112.64940256557722 Iterations theta0 0.031170784841389347 theta1 1.4788794530640061\n",
      "112.64936645702217 Iterations theta0 0.03123086374644001 theta1 1.4788782722554572\n",
      "112.64933034901209 Iterations theta0 0.03129094219779529 theta1 1.478877091443852\n",
      "112.64929424154703 Iterations theta0 0.031351020195575854 theta1 1.4788759106351557\n",
      "112.64925813462713 Iterations theta0 0.031411097739843935 theta1 1.4788747298323603\n",
      "112.64922202825228 Iterations theta0 0.03147117483063245 theta1 1.4788735490369667\n",
      "112.6491859224223 Iterations theta0 0.031531251467959626 theta1 1.4788723682497276\n",
      "112.64914981713753 Iterations theta0 0.0315913276518363 theta1 1.4788711874710208\n",
      "112.64911371239778 Iterations theta0 0.03165140338226963 theta1 1.4788700067010354\n",
      "112.64907760820297 Iterations theta0 0.031711478659264894 theta1 1.4788688259398666\n",
      "112.64904150455321 Iterations theta0 0.03177155348282646 theta1 1.478867645187562\n",
      "112.64900540144839 Iterations theta0 0.03183162785295821 theta1 1.4788664644441452\n",
      "112.64896929888867 Iterations theta0 0.03189170176966381 theta1 1.4788652837096283\n",
      "112.64893319687393 Iterations theta0 0.0319517752329468 theta1 1.4788641029840173\n",
      "112.64889709540407 Iterations theta0 0.03201184824281067 theta1 1.478862922267315\n",
      "112.64886099447925 Iterations theta0 0.03207192079925886 theta1 1.4788617415595229\n",
      "112.64882489409928 Iterations theta0 0.032131992902294806 theta1 1.4788605608606418\n",
      "112.64878879426435 Iterations theta0 0.032192064551921945 theta1 1.478859380170672\n",
      "112.6487526949743 Iterations theta0 0.032252135748143694 theta1 1.4788581994896135\n",
      "112.64871659622933 Iterations theta0 0.03231220649096349 theta1 1.4788570188174663\n",
      "112.6486804980292 Iterations theta0 0.03237227678038474 theta1 1.4788558381542305\n",
      "112.64864440037395 Iterations theta0 0.03243234661641088 theta1 1.478854657499906\n",
      "112.64860830326369 Iterations theta0 0.03249241599904533 theta1 1.4788534768544928\n",
      "112.64857220669826 Iterations theta0 0.032552484928291506 theta1 1.4788522962179906\n",
      "112.64853611067775 Iterations theta0 0.03261255340415284 theta1 1.4788511155903998\n",
      "112.64850001520203 Iterations theta0 0.03267262142663274 theta1 1.4788499349717201\n",
      "112.64846392027134 Iterations theta0 0.03273268899573464 theta1 1.4788487543619513\n",
      "112.64842782588546 Iterations theta0 0.032792756111461964 theta1 1.4788475737610933\n",
      "112.64839173204442 Iterations theta0 0.032852822773818124 theta1 1.4788463931691462\n",
      "112.64835563874827 Iterations theta0 0.032912888982806546 theta1 1.47884521258611\n",
      "112.64831954599687 Iterations theta0 0.032972954738430656 theta1 1.4788440320119847\n",
      "112.64828345379041 Iterations theta0 0.03303302004069387 theta1 1.47884285144677\n",
      "112.6482473621288 Iterations theta0 0.03309308488959961 theta1 1.4788416708904657\n",
      "112.64821127101195 Iterations theta0 0.033153149285151305 theta1 1.478840490343072\n",
      "112.64817518043988 Iterations theta0 0.033213213227352364 theta1 1.4788393098045889\n",
      "112.64813909041263 Iterations theta0 0.033273276716206224 theta1 1.4788381292750161\n",
      "112.64810300093023 Iterations theta0 0.0333333397517163 theta1 1.4788369487543538\n",
      "112.64806691199259 Iterations theta0 0.033393402333886005 theta1 1.4788357682426014\n",
      "112.64803082359973 Iterations theta0 0.033453464462718775 theta1 1.4788345877397595\n",
      "112.6479947357516 Iterations theta0 0.03351352613821802 theta1 1.4788334072458276\n",
      "112.64795864844824 Iterations theta0 0.03357358736038717 theta1 1.4788322267608058\n",
      "112.64792256168965 Iterations theta0 0.03363364812922964 theta1 1.478831046284694\n",
      "112.64788647547576 Iterations theta0 0.03369370844474886 theta1 1.4788298658174923\n",
      "112.64785038980665 Iterations theta0 0.03375376830694824 theta1 1.4788286853592003\n",
      "112.64781430468229 Iterations theta0 0.03381382771583121 theta1 1.478827504909818\n",
      "112.64777822010262 Iterations theta0 0.03387388667140119 theta1 1.4788263244693456\n",
      "112.64774213606768 Iterations theta0 0.033933945173661606 theta1 1.4788251440377829\n",
      "112.64770605257742 Iterations theta0 0.03399400322261587 theta1 1.4788239636151297\n",
      "112.64766996963193 Iterations theta0 0.03405406081826741 theta1 1.478822783201386\n",
      "112.64763388723102 Iterations theta0 0.034114117960619646 theta1 1.4788216027965517\n",
      "112.6475978053749 Iterations theta0 0.034174174649676 theta1 1.478820422400627\n",
      "112.64756172406332 Iterations theta0 0.03423423088543989 theta1 1.4788192420136113\n",
      "112.64752564329656 Iterations theta0 0.034294286667914745 theta1 1.4788180616355049\n",
      "112.64748956307429 Iterations theta0 0.03435434199710398 theta1 1.4788168812663078\n",
      "112.6474534833968 Iterations theta0 0.03441439687301102 theta1 1.47881570090602\n",
      "112.64741740426382 Iterations theta0 0.03447445129563927 theta1 1.4788145205546412\n",
      "112.64738132567561 Iterations theta0 0.034534505264992174 theta1 1.4788133402121713\n",
      "112.64734524763193 Iterations theta0 0.03459455878107314 theta1 1.4788121598786104\n",
      "112.64730917013289 Iterations theta0 0.034654611843885595 theta1 1.4788109795539583\n",
      "112.64727309317841 Iterations theta0 0.03471466445343296 theta1 1.478809799238215\n",
      "112.64723701676857 Iterations theta0 0.03477471660971865 theta1 1.4788086189313803\n",
      "112.6472009409034 Iterations theta0 0.03483476831274609 theta1 1.4788074386334544\n",
      "112.64716486558272 Iterations theta0 0.0348948195625187 theta1 1.478806258344437\n",
      "112.64712879080662 Iterations theta0 0.03495487035903991 theta1 1.478805078064328\n",
      "112.64709271657507 Iterations theta0 0.03501492070231313 theta1 1.4788038977931277\n",
      "112.64705664288809 Iterations theta0 0.035074970592341784 theta1 1.4788027175308358\n",
      "112.64702056974568 Iterations theta0 0.03513502002912929 theta1 1.4788015372774521\n",
      "112.64698449714773 Iterations theta0 0.035195069012679076 theta1 1.4788003570329766\n",
      "112.64694842509438 Iterations theta0 0.03525511754299456 theta1 1.4787991767974094\n",
      "112.64691235358552 Iterations theta0 0.035315165620079164 theta1 1.4787979965707503\n",
      "112.64687628262124 Iterations theta0 0.035375213243936304 theta1 1.4787968163529992\n",
      "112.64684021220145 Iterations theta0 0.0354352604145694 theta1 1.478795636144156\n",
      "112.6468041423261 Iterations theta0 0.03549530713198189 theta1 1.4787944559442208\n",
      "112.64676807299533 Iterations theta0 0.03555535339617717 theta1 1.4787932757531934\n",
      "112.64673200420897 Iterations theta0 0.03561539920715868 theta1 1.4787920955710738\n",
      "112.64669593596709 Iterations theta0 0.035675444564929826 theta1 1.478790915397862\n",
      "112.64665986826967 Iterations theta0 0.03573548946949404 theta1 1.4787897352335577\n",
      "112.64662380111675 Iterations theta0 0.035795533920854744 theta1 1.4787885550781612\n",
      "112.64658773450829 Iterations theta0 0.03585557791901534 theta1 1.478787374931672\n",
      "112.6465516684441 Iterations theta0 0.03591562146397927 theta1 1.4787861947940903\n",
      "112.64651560292452 Iterations theta0 0.03597566455574995 theta1 1.478785014665416\n",
      "112.64647953794936 Iterations theta0 0.036035707194330795 theta1 1.478783834545649\n",
      "112.64644347351859 Iterations theta0 0.03609574937972523 theta1 1.4787826544347893\n",
      "112.64640740963219 Iterations theta0 0.03615579111193667 theta1 1.4787814743328367\n",
      "112.64637134629021 Iterations theta0 0.03621583239096853 theta1 1.4787802942397912\n",
      "112.6463352834926 Iterations theta0 0.036275873216824246 theta1 1.4787791141556528\n",
      "112.64629922123945 Iterations theta0 0.03633591358950723 theta1 1.4787779340804212\n",
      "112.64626315953065 Iterations theta0 0.03639595350902091 theta1 1.4787767540140966\n",
      "112.64622709836617 Iterations theta0 0.036455992975368695 theta1 1.4787755739566788\n",
      "112.64619103774604 Iterations theta0 0.036516031988554014 theta1 1.478774393908168\n",
      "112.64615497767028 Iterations theta0 0.03657607054858028 theta1 1.4787732138685636\n",
      "112.64611891813887 Iterations theta0 0.03663610865545092 theta1 1.478772033837866\n",
      "112.64608285915183 Iterations theta0 0.036696146309169356 theta1 1.4787708538160749\n",
      "112.64604680070912 Iterations theta0 0.036756183509739 theta1 1.4787696738031904\n",
      "112.64601074281069 Iterations theta0 0.036816220257163274 theta1 1.4787684937992123\n",
      "112.64597468545651 Iterations theta0 0.0368762565514456 theta1 1.4787673138041404\n",
      "112.64593862864669 Iterations theta0 0.0369362923925894 theta1 1.478766133817975\n",
      "112.64590257238123 Iterations theta0 0.036996327780598096 theta1 1.4787649538407157\n",
      "112.64586651665992 Iterations theta0 0.037056362715475105 theta1 1.4787637738723627\n",
      "112.64583046148299 Iterations theta0 0.037116397197223844 theta1 1.4787625939129156\n",
      "112.64579440685031 Iterations theta0 0.03717643122584774 theta1 1.4787614139623748\n",
      "112.64575835276186 Iterations theta0 0.037236464801350205 theta1 1.4787602340207397\n",
      "112.64572229921764 Iterations theta0 0.03729649792373467 theta1 1.4787590540880107\n",
      "112.64568624621779 Iterations theta0 0.03735653059300455 theta1 1.4787578741641874\n",
      "112.645650193762 Iterations theta0 0.03741656280916326 theta1 1.47875669424927\n",
      "112.64561414185059 Iterations theta0 0.037476594572214215 theta1 1.4787555143432582\n",
      "112.6455780904833 Iterations theta0 0.03753662588216085 theta1 1.478754334446152\n",
      "112.64554203966023 Iterations theta0 0.03759665673900658 theta1 1.4787531545579515\n",
      "112.64550598938142 Iterations theta0 0.037656687142754816 theta1 1.4787519746786564\n",
      "112.64546993964676 Iterations theta0 0.03771671709340899 theta1 1.4787507948082665\n",
      "112.64543389045627 Iterations theta0 0.03777674659097252 theta1 1.4787496149467823\n",
      "112.64539784181001 Iterations theta0 0.03783677563544882 theta1 1.4787484350942033\n",
      "112.64536179370792 Iterations theta0 0.037896804226841316 theta1 1.4787472552505294\n",
      "112.64532574614992 Iterations theta0 0.037956832365153424 theta1 1.4787460754157609\n",
      "112.6452896991361 Iterations theta0 0.03801686005038856 theta1 1.4787448955898972\n",
      "112.64525365266653 Iterations theta0 0.03807688728255015 theta1 1.4787437157729386\n",
      "112.64521760674101 Iterations theta0 0.03813691406164161 theta1 1.4787425359648851\n",
      "112.64518156135962 Iterations theta0 0.03819694038766636 theta1 1.4787413561657365\n",
      "112.64514551652229 Iterations theta0 0.03825696626062782 theta1 1.4787401763754926\n",
      "112.64510947222911 Iterations theta0 0.03831699168052941 theta1 1.4787389965941535\n",
      "112.64507342848009 Iterations theta0 0.03837701664737455 theta1 1.4787378168217191\n",
      "112.64503738527512 Iterations theta0 0.03843704116116666 theta1 1.4787366370581894\n",
      "112.64500134261426 Iterations theta0 0.03849706522190916 theta1 1.478735457303564\n",
      "112.64496530049745 Iterations theta0 0.03855708882960547 theta1 1.4787342775578434\n",
      "112.64492925892475 Iterations theta0 0.038617111984259 theta1 1.478733097821027\n",
      "112.64489321789611 Iterations theta0 0.03867713468587319 theta1 1.4787319180931149\n",
      "112.64485717741151 Iterations theta0 0.038737156934451435 theta1 1.478730738374107\n",
      "112.64482113747094 Iterations theta0 0.038797178729997175 theta1 1.4787295586640037\n",
      "112.64478509807446 Iterations theta0 0.038857200072513816 theta1 1.4787283789628043\n",
      "112.644749059222 Iterations theta0 0.038917220962004784 theta1 1.478727199270509\n",
      "112.64471302091347 Iterations theta0 0.03897724139847349 theta1 1.4787260195871177\n",
      "112.64467698314903 Iterations theta0 0.03903726138192337 theta1 1.4787248399126305\n",
      "112.64464094592863 Iterations theta0 0.039097280912357826 theta1 1.478723660247047\n",
      "112.64460490925214 Iterations theta0 0.039157299989780285 theta1 1.4787224805903674\n",
      "112.64456887311977 Iterations theta0 0.03921731861419416 theta1 1.4787213009425915\n",
      "112.6445328375312 Iterations theta0 0.039277336785602884 theta1 1.4787201213037193\n",
      "112.64449680248669 Iterations theta0 0.03933735450400987 theta1 1.4787189416737507\n",
      "112.6444607679862 Iterations theta0 0.03939737176941852 theta1 1.4787177620526857\n",
      "112.64442473402961 Iterations theta0 0.03945738858183228 theta1 1.478716582440524\n",
      "112.64438870061689 Iterations theta0 0.03951740494125455 theta1 1.4787154028372658\n",
      "112.64435266774825 Iterations theta0 0.03957742084768876 theta1 1.4787142232429111\n",
      "112.64431663542345 Iterations theta0 0.03963743630113833 theta1 1.4787130436574596\n",
      "112.64428060364267 Iterations theta0 0.039697451301606665 theta1 1.4787118640809112\n",
      "112.6442445724057 Iterations theta0 0.039757465849097194 theta1 1.478710684513266\n",
      "112.6442085417127 Iterations theta0 0.039817479943613336 theta1 1.478709504954524\n",
      "112.64417251156355 Iterations theta0 0.03987749358515851 theta1 1.4787083254046849\n",
      "112.6441364819583 Iterations theta0 0.039937506773736135 theta1 1.4787071458637486\n",
      "112.64410045289705 Iterations theta0 0.03999751950934963 theta1 1.4787059663317152\n",
      "112.64406442437956 Iterations theta0 0.04005753179200241 theta1 1.4787047868085847\n",
      "112.64402839640596 Iterations theta0 0.04011754362169789 theta1 1.478703607294357\n",
      "112.64399236897627 Iterations theta0 0.0401775549984395 theta1 1.4787024277890317\n",
      "112.64395634209028 Iterations theta0 0.04023756592223066 theta1 1.4787012482926094\n",
      "112.64392031574826 Iterations theta0 0.04029757639307478 theta1 1.4787000688050895\n",
      "112.64388428994998 Iterations theta0 0.04035758641097528 theta1 1.478698889326472\n",
      "112.6438482646956 Iterations theta0 0.04041759597593558 theta1 1.4786977098567569\n",
      "112.64381223998498 Iterations theta0 0.040477605087959105 theta1 1.478696530395944\n",
      "112.64377621581824 Iterations theta0 0.04053761374704926 theta1 1.4786953509440337\n",
      "112.64374019219524 Iterations theta0 0.04059762195320948 theta1 1.4786941715010253\n",
      "112.64370416911612 Iterations theta0 0.04065762970644317 theta1 1.4786929920669192\n",
      "112.64366814658071 Iterations theta0 0.04071763700675375 theta1 1.4786918126417152\n",
      "112.64363212458908 Iterations theta0 0.040777643854144646 theta1 1.4786906332254133\n",
      "112.64359610314125 Iterations theta0 0.04083765024861927 theta1 1.478689453818013\n",
      "112.64356008223712 Iterations theta0 0.040897656190181035 theta1 1.4786882744195147\n",
      "112.6435240618768 Iterations theta0 0.04095766167883337 theta1 1.4786870950299182\n",
      "112.64348804206018 Iterations theta0 0.041017666714579695 theta1 1.4786859156492236\n",
      "112.64345202278727 Iterations theta0 0.04107767129742342 theta1 1.4786847362774305\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "def costFunction(theta0,theta1,data):\n",
    "   \n",
    "    totalError=0\n",
    "    for i in range(0, len(data)):\n",
    "         y=data[i,1]\n",
    "         x=data[i,0]\n",
    "         totalError+=(y-(theta1*x+theta0))**2\n",
    "    return totalError/float(len(data))\n",
    "    \n",
    "def gradientDescent(theta0,theta1,data,learningRate):\n",
    "    derivativetheta0=0\n",
    "    derivativetheta1=0\n",
    "    for i in range(0,len(data)):\n",
    "        y=data[i,1]\n",
    "        x=data[i,0]\n",
    "        derivativetheta0+=-(2*(y-(theta1*x+theta0)))/len(data)\n",
    "        derivativetheta1+=-2*x*(y-(theta1*x+theta0))/len(data)\n",
    "    theta0=theta0-(learningRate*derivativetheta0)\n",
    "    theta1=theta1-(learningRate*derivativetheta1)\n",
    "    return[theta0,theta1]\n",
    "\n",
    "def gradientDescentConverger(theta0,theta1,data,leraningrate,iterations):\n",
    "   \n",
    "    for i in range(0,iterations):\n",
    "     [theta0, theta1]=gradientDescent(theta0,theta1,data,leraningrate)\n",
    "     print(costFunction(theta0,theta1,data),\"Iterations\",\"theta0\",theta0,\"theta1\",theta1)\n",
    "    \n",
    "    return[theta0,theta1]\n",
    "    \n",
    "        \n",
    "def run():\n",
    "    data=genfromtxt(\"Lineardata.csv\", delimiter=\",\")\n",
    "    learningRate=0.0001\n",
    "    theta0=0\n",
    "    theta1=0\n",
    "    iterations=200\n",
    "    costFunction(theta0,theta1,data)\n",
    "    [theta0, theta1]=gradientDescentConverger(theta0,theta1,data,learningRate,iterations)\n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/gradient_descent_error_by_iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
